{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8701e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import io, utils\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import tv_tensors\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cd741",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a014f13",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\"\"\n",
    "Data_folder\n",
    "    -train\n",
    "        -images\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            .\n",
    "            .\n",
    "        -labels\n",
    "            img1.txt\n",
    "            img2.txt\n",
    "            .\n",
    "            .\n",
    "    -val\n",
    "        -images\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            .\n",
    "            .\n",
    "        -labels\n",
    "            img1.txt\n",
    "            img2.txt\n",
    "            .\n",
    "            .\n",
    "    -test\n",
    "        -images\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            .\n",
    "            .\n",
    "        -labels\n",
    "            img1.txt\n",
    "            img2.txt\n",
    "            .\n",
    "            .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pascal_voc format and .png image only (CV2 and Albumentation)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, split, image_type, transforms=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.image_names = os.listdir(os.path.join(self.root, self.split, 'images'))\n",
    "        self.label_names = [name.replace(f'.{image_type}', '.txt') for name in self.image_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.root, self.split, 'images', self.image_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "#         print(image)\n",
    "        if image is None:\n",
    "            raise Exception(f\"Error reading image: {image_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "        # Load bounding boxes and labels\n",
    "        label_path = os.path.join(self.root, self.split, 'labels', self.label_names[idx])\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.read().split('\\n')\n",
    "#         print(lines)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i].split()\n",
    "\n",
    "            boxes.append([\n",
    "                float(line[1]),\n",
    "                float(line[2]),\n",
    "                float(line[3]),\n",
    "                float(line[4]),\n",
    "            ])\n",
    "            labels.append(int(line[0])+1)\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = augmented['image']\n",
    "            boxes = [list(box) for box in augmented['bboxes']]\n",
    "            labels = augmented['labels']\n",
    "            \n",
    "        heigth, width, _ = image.shape\n",
    "#         # Convert image to torch tensor\n",
    "        image = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYWH\", canvas_size=(heigth, width))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "#         # suppose all instances are not crowd\n",
    "        iscrowd = torch.tensor([0]*len(labels), dtype=torch.int64)\n",
    "    \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = idx\n",
    "        try:\n",
    "            target[\"area\"] = boxes[:, 2] * boxes[:, 3]\n",
    "        except IndexError:\n",
    "            target[\"area\"] = torch.tensor([], dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "            \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_box_target(image, target, color='blue', width=2, figsize=(8, 8)):\n",
    "    boxes = target['boxes']\n",
    "    \n",
    "    result = draw_bounding_boxes(image, boxes, colors=color, width=width)\n",
    "    result = T.ToPILImage()(result)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(result)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfebec4",
   "metadata": {},
   "source": [
    "## augment and show a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d572ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                                scale_limit=0.1, \n",
    "                                rotate_limit=15, \n",
    "                                border_mode=cv2.BORDER_CONSTANT, \n",
    "                                value=0,\n",
    "                                p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=180*0.1,\n",
    "                                 sat_shift_limit=255*0.05, \n",
    "                                 val_shift_limit=255*0.05, \n",
    "                                 p=0.5),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    "\n",
    "train_data = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    image, target = train_data[i]\n",
    "#     print(target)\n",
    "#     print(image)\n",
    "    visualize_box_target(image, target, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536fac1",
   "metadata": {},
   "source": [
    "## augmentation setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed75bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentation\n",
    "def get_transform(train):\n",
    "    if train:\n",
    "        transforms = [\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                                scale_limit=0.1, \n",
    "                                rotate_limit=15, \n",
    "                                border_mode=cv2.BORDER_CONSTANT, \n",
    "                                value=0,\n",
    "                                p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=180*0.1,\n",
    "                                 sat_shift_limit=255*0.05, \n",
    "                                 val_shift_limit=255*0.05, \n",
    "                                 p=0.5),\n",
    "            \n",
    "            A.Normalize()\n",
    "                    ]\n",
    "    else:\n",
    "        transforms = [\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.Normalize()\n",
    "        ]\n",
    "        \n",
    "    return A.Compose(transforms, \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595307c",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# our dataset has two classes only - background and cabbage\n",
    "num_classes = 2\n",
    "batch_size = 16\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "# define training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc61475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}')    # amount of steps for training\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b4496",
   "metadata": {},
   "source": [
    "## Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc20f56",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, device):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(src, num_classes, device):\n",
    "    model = get_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ef809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the model using our helper function\n",
    "model = get_model(num_classes, device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 512, 512), depth=3, col_names = (\"num_params\", \"mult_adds\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3053a5d",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d33bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "def train(model, optimizer, device, num_epochs, train_loader, val_loader, project, name, print_freq):\n",
    "    best_mAP = -1.00  # Initialize best_mAP to a very low value\n",
    "    \n",
    "    # --- result saving dir ---\n",
    "    os.makedirs('runs', exist_ok=True)\n",
    "    os.makedirs(f'runs/{project}', exist_ok=True)\n",
    "    os.makedirs(f'runs/{project}/{name}')\n",
    "    best_model_path = f'runs/{project}/{name}/best.pth'\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # train for one epoch, printing every {print_freq} iterations\n",
    "        train_result = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=print_freq)\n",
    "        # get training loss\n",
    "        epoch_loss = train_result.get_meter('epoch_loss').avg\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on the val dataset (metrix beased)\n",
    "        val_result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)\n",
    "        # get mAP@50:95\n",
    "        mAP = val_result.coco_eval['bbox'].stats[0]\n",
    "\n",
    "        # Check if the current model is the best\n",
    "        if best_mAP < mAP:\n",
    "            best_mAP = mAP\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved new best model with mAP: {mAP:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cff2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_freq = 5\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='faster_rcnn',\n",
    "    name='500ep_aug',\n",
    "    print_freq=print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7f584",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733cf9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('runs/faster_rcnn/500ep_aug/best.pth', num_classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}') \n",
    "\n",
    "# evaluate on the train dataset\n",
    "result = evaluate(model, train_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbca64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}') \n",
    "\n",
    "# evaluate on the valid dataset\n",
    "result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Testing set: #image = {len(test_dataset)}, batch size = {len(test_loader)}') \n",
    "\n",
    "# evaluate on the test dataset\n",
    "result = evaluate(model, test_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a39886",
   "metadata": {},
   "source": [
    "## model predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def predict(model, image, device, iou_threshold=0.6):\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(size=512),\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.ToPureTensor()\n",
    "    ])\n",
    "    image = transforms(image)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = image\n",
    "        # convert RGBA -> RGB and move to device\n",
    "        x = x[:3, ...].to(device)\n",
    "        predictions = model([x, ])\n",
    "        pred = predictions[0]\n",
    "\n",
    "    # denormalize a image\n",
    "    image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "    image = image[:3, ...]\n",
    "    \n",
    "    return image, pred\n",
    "\n",
    "def visualize_box_result(i, image, result, color='red', width=2, figsize=(8, 8), axis='off', save=False, dst=None):\n",
    "    # visualize a result\n",
    "    if result[\"boxes\"].shape[0] == 0:\n",
    "        output_image = image\n",
    "    else:\n",
    "        result_labels = [f\"cabbage: {score:.3f}\" for label, score in zip(result[\"labels\"], result[\"scores\"])]\n",
    "        result_boxes = result[\"boxes\"].long()\n",
    "        output_image = draw_bounding_boxes(image, result_boxes, result_labels, colors=color, width=width)\n",
    "    output_image = output_image.permute(1, 2, 0)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(output_image)\n",
    "    plt.axis(axis)\n",
    "    \n",
    "#     print(output_image.numpy())\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "    plt.imsave(f'{dst}/image{i+1}.png', output_image.numpy())\n",
    "\n",
    "def visualize_box_target_result(image, target, result, color=('blue', 'red'), width=2, figsize=(12, 12), axis='on'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # target\n",
    "    target_box = draw_bounding_boxes(image, target['boxes'], colors=color[0], width=width)\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(target_box.permute(1, 2, 0))\n",
    "    plt.axis(axis)\n",
    "    \n",
    "    # result\n",
    "    if result[\"boxes\"].shape[0] == 0:\n",
    "        output_box = image\n",
    "    else:\n",
    "        result_labels = [f\"cabbage: {score:.3f}\" for label, score in zip(result[\"labels\"], result[\"scores\"])]\n",
    "        result_boxes = result[\"boxes\"].long()\n",
    "        output_box = draw_bounding_boxes(image, result_boxes, result_labels, colors=color[1], width=width, font_size=30)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(output_box.permute(1, 2, 0))\n",
    "    plt.axis(axis)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def visualize_box_both(image, target, result, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # target\n",
    "    target_box = draw_bounding_boxes(image, target['boxes'], colors=color[0], width=width)\n",
    "    \n",
    "    # result\n",
    "    result_labels = [f\"cabbage: {score:.3f}\" for label, score in zip(result[\"labels\"], result[\"scores\"])]\n",
    "    result_boxes = result[\"boxes\"].long()\n",
    "    output_box = draw_bounding_boxes(target_box, result_boxes, result_labels, colors=color[1], width=width, font_size=30)\n",
    "    \n",
    "    plt.imshow(output_box.permute(1, 2, 0))\n",
    "    plt.axis(axis)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d76bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from engine import post_processing\n",
    "\n",
    "classId2name = {\n",
    "    0: 'background',\n",
    "    1: 'cabbage'\n",
    "}\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=A.Compose([A.Resize(height=512, width=512, p=1)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    ")\n",
    "\n",
    "confident_threshold = 0.5\n",
    "area_threshold = 100\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, target = test_dataset[i]\n",
    "    \n",
    "    image, pred = predict(model, image, device)\n",
    "    \n",
    "    # post processing\n",
    "    pred = post_processing([pred], confident_threshold, area_threshold)[0]\n",
    "    \n",
    "    # visualize a result\n",
    "    visualize_box_result(i, image, pred, color='red', width=2, figsize=(8, 8), axis='off', save=True, dst='runs/faster_rcnn/500ep_aug/test_results')\n",
    "    \n",
    "    # visualize target and result\n",
    "#     visualize_box_target_result(image, target, pred, color=('blue', 'red'), width=2, figsize=(12, 12), axis='off')   \n",
    "#     visualize_box_both(image, target, pred, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9cdcb",
   "metadata": {},
   "source": [
    "# RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02965f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, device):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.retinanet.retinanet_resnet50_fpn(weights=\"COCO_V1\")\n",
    "\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    in_channels = model.head.classification_head.conv[0][0].in_channels\n",
    "    \n",
    "    model.head.classification_head = RetinaNetClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes,\n",
    "#         norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(src, num_classes, device):\n",
    "    model = get_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969dd585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes, device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 512, 512), depth=3, col_names = (\"num_params\", \"mult_adds\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ba604",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c4f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_freq = 5\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='retinalnet',\n",
    "    name='500ep_aug',\n",
    "    print_freq=print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133cee4",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('runs/retinalnet/500ep_aug/best.pth', num_classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}') \n",
    "\n",
    "# evaluate on the train dataset\n",
    "result = evaluate(model, train_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}') \n",
    "\n",
    "# evaluate on the valid dataset\n",
    "result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a380933",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Testing set: #image = {len(test_dataset)}, batch size = {len(test_loader)}') \n",
    "\n",
    "# evaluate on the test dataset\n",
    "result = evaluate(model, test_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bee03",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc82c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from engine import post_processing\n",
    "\n",
    "classId2name = {\n",
    "    0: 'background',\n",
    "    1: 'cabbage'\n",
    "}\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=A.Compose([A.Resize(height=512, width=512, p=1)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    ")\n",
    "\n",
    "confident_threshold = 0.5\n",
    "area_threshold = 100\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, target = test_dataset[i]\n",
    "    \n",
    "    image, pred = predict(model, image, device)\n",
    "    \n",
    "    # post processing\n",
    "    pred = post_processing([pred], confident_threshold, area_threshold)[0]\n",
    "    \n",
    "    # visualize a result\n",
    "    visualize_box_result(i, image, pred, color='red', width=2, figsize=(8, 8), axis='off', save=True, dst='runs/retinalnet/500ep_aug/test_results')\n",
    "    \n",
    "    # visualize target and result\n",
    "#     visualize_box_target_result(image, target, pred, color=('blue', 'red'), width=2, figsize=(12, 12), axis='off')   \n",
    "#     visualize_box_both(image, target, pred, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a467a71",
   "metadata": {},
   "source": [
    "# SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e777f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import _utils\n",
    "import torch\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ace8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(device, num_classes=91, size=300):\n",
    "    # Load the Torchvision pretrained model.\n",
    "    model = ssd300_vgg16(weights=\"COCO_V1\")\n",
    "    \n",
    "    # Retrieve the list of input channels. \n",
    "    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n",
    "    # List containing number of anchors based on aspect ratios.\n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "    # The classification head.\n",
    "    model.head.classification_head = SSDClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    # Image size for transforms.\n",
    "    model.transform.min_size = (size,)\n",
    "    model.transform.max_size = size\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(src, num_classes, device, size):\n",
    "    model = get_model(num_classes=num_classes, device=device, size=size)\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10926a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes=num_classes, device=device, size=512)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 512, 512), depth=3, col_names = (\"num_params\", \"mult_adds\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c1316",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc2666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_freq = 5\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='ssd',\n",
    "    name='500ep_aug',\n",
    "    print_freq=print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d77324",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('runs/ssd/500ep_aug/best.pth', num_classes, device, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}') \n",
    "\n",
    "# evaluate on the train dataset\n",
    "result = evaluate(model, train_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}') \n",
    "\n",
    "# evaluate on the valid dataset\n",
    "result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Testing set: #image = {len(test_dataset)}, batch size = {len(test_loader)}') \n",
    "\n",
    "# evaluate on the test dataset\n",
    "result = evaluate(model, test_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed9907",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096a956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from engine import post_processing\n",
    "\n",
    "classId2name = {\n",
    "    0: 'background',\n",
    "    1: 'cabbage'\n",
    "}\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=A.Compose([A.Resize(height=512, width=512, p=1)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    ")\n",
    "\n",
    "confident_threshold = 0.5\n",
    "area_threshold = 100\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, target = test_dataset[i]\n",
    "    \n",
    "    image, pred = predict(model, image, device)\n",
    "    \n",
    "    # post processing\n",
    "    pred = post_processing([pred], confident_threshold, area_threshold)[0]\n",
    "    \n",
    "    # visualize a result\n",
    "    visualize_box_result(i, image, pred, color='red', width=2, figsize=(8, 8), axis='off', save=True, dst='runs/ssd/500ep_aug/test_results')\n",
    "    \n",
    "    # visualize target and result\n",
    "#     visualize_box_target_result(image, target, pred, color=('blue', 'red'), width=2, figsize=(12, 12), axis='off')   \n",
    "#     visualize_box_both(image, target, pred, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56296e9d",
   "metadata": {},
   "source": [
    "# SSDlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torchvision.models.detection import _utils\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(device, num_classes=91, size=320):\n",
    "    # Load the Torchvision pretrained model.\n",
    "    model = ssdlite320_mobilenet_v3_large(weights=\"COCO_V1\")\n",
    "    \n",
    "    # Retrieve the list of input channels. \n",
    "    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n",
    "    # List containing number of anchors based on aspect ratios.\n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "    # The classification head.\n",
    "    model.head.classification_head = SSDLiteClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes,\n",
    "        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n",
    "    )\n",
    "    # Image size for transforms.\n",
    "    model.transform.min_size = (size,)\n",
    "    model.transform.max_size = size\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(src, num_classes, device, size):\n",
    "    model = get_model(num_classes=num_classes, device=device, size=size)\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88276433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes=num_classes, device=device, size=512)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 512, 512), depth=3, col_names = (\"num_params\", \"mult_adds\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b967c",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43912f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_freq = 5\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='ssdlite',\n",
    "    name='500ep_aug',\n",
    "    print_freq=print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5f117",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('runs/ssdlite/500ep_aug/best.pth', num_classes, device, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}') \n",
    "\n",
    "# evaluate on the train dataset\n",
    "result = evaluate(model, train_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}') \n",
    "\n",
    "# evaluate on the valid dataset\n",
    "result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Testing set: #image = {len(test_dataset)}, batch size = {len(test_loader)}') \n",
    "\n",
    "# evaluate on the test dataset\n",
    "result = evaluate(model, test_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a37d2a",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3e1b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from engine import post_processing\n",
    "\n",
    "classId2name = {\n",
    "    0: 'background',\n",
    "    1: 'cabbage'\n",
    "}\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=A.Compose([A.Resize(height=512, width=512, p=1)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    ")\n",
    "\n",
    "confident_threshold = 0.5\n",
    "area_threshold = 100\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, target = test_dataset[i]\n",
    "    \n",
    "    image, pred = predict(model, image, device)\n",
    "    \n",
    "    # post processing\n",
    "    pred = post_processing([pred], confident_threshold, area_threshold)[0]\n",
    "    \n",
    "    # visualize a result\n",
    "    visualize_box_result(i, image, pred, color='red', width=2, figsize=(8, 8), axis='off', save=True, dst='runs/ssdlite/500ep_aug/test_results')\n",
    "    \n",
    "    # visualize target and result\n",
    "#     visualize_box_target_result(image, target, pred, color=('blue', 'red'), width=2, figsize=(12, 12), axis='off')   \n",
    "#     visualize_box_both(image, target, pred, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da59e3",
   "metadata": {},
   "source": [
    "# FCOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.detection import fcos_resnet50_fpn\n",
    "from torchvision.models.detection.fcos  import FCOSClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, device):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = fcos_resnet50_fpn(weights=\"COCO_V1\")\n",
    "\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    in_channels = model.head.classification_head.conv[0].in_channels\n",
    "    \n",
    "    model.head.classification_head = FCOSClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(src, num_classes, device):\n",
    "    model = get_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef52fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes, device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 512, 512), depth=3, col_names = (\"num_params\", \"mult_adds\"), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa1c8c",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acf99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_freq = 5\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    project='fcos',\n",
    "    name='500ep_aug',\n",
    "    print_freq=print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a379f",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('runs/fcos/500ep_aug/best.pth', num_classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='train',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Training set: #image = {len(train_dataset)}, batch size = {len(train_loader)}') \n",
    "\n",
    "# evaluate on the train dataset\n",
    "result = evaluate(model, train_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b18b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='val',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Validation set: #image = {len(val_dataset)}, batch size = {len(val_loader)}') \n",
    "\n",
    "# evaluate on the valid dataset\n",
    "result = evaluate(model, val_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(f'Testing set: #image = {len(test_dataset)}, batch size = {len(test_loader)}') \n",
    "\n",
    "# evaluate on the test dataset\n",
    "result = evaluate(model, test_loader, device=device, confident_threshold=0.5, area_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21123bca",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7a5e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from engine import post_processing\n",
    "\n",
    "classId2name = {\n",
    "    0: 'background',\n",
    "    1: 'cabbage'\n",
    "}\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    root='data/fold1_pascal',\n",
    "    split='test',\n",
    "    image_type='png',\n",
    "    transforms=A.Compose([A.Resize(height=512, width=512, p=1)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.0, min_area=100))\n",
    ")\n",
    "\n",
    "confident_threshold = 0.5\n",
    "area_threshold = 100\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, target = test_dataset[i]\n",
    "    \n",
    "    image, pred = predict(model, image, device)\n",
    "    \n",
    "    # post processing\n",
    "    pred = post_processing([pred], confident_threshold, area_threshold)[0]\n",
    "    \n",
    "    # visualize a result\n",
    "    visualize_box_result(i, image, pred, color='red', width=2, figsize=(8, 8), axis='off', save=True, dst='runs/fcos/500ep_aug/test_results')\n",
    "    \n",
    "    # visualize target and result\n",
    "#     visualize_box_target_result(image, target, pred, color=('blue', 'red'), width=2, figsize=(12, 12), axis='off')   \n",
    "#     visualize_box_both(image, target, pred, color=('blue', 'red'), width=2, figsize=(8, 8), axis='on')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test01",
   "language": "python",
   "name": "test01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
